{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing all Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing custom functions\n",
    "import sys\n",
    "sys.path.append('/Users/ristomartin/Dropbox/UniStuff/DPhil/Experimental/python_analysis/common_functions')\n",
    "from custom_functions import *\n",
    "from common_fits import *\n",
    "\n",
    "#importing standard functions\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import iqr\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from statannot import add_stat_annotation\n",
    "import itertools\n",
    "\n",
    "from scipy.optimize import curve_fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generation of raw datasets from image analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting location of directories and constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/ristomartin/Dropbox/UniStuff/DPhil/Experimental/python_analysis/micro_fibre_dists/fibre_diameter\n"
     ]
    }
   ],
   "source": [
    "#Root location\n",
    "root = '/Users/ristomartin/Dropbox/UniStuff/DPhil/Experimental/'\n",
    "#current location\n",
    "location = os.getcwd()\n",
    "print(location)\n",
    "\n",
    "#Data set locations\n",
    "#Hollow fibre micro-fibre analysis\n",
    "#Effect of polymer concentration & Pyridine concentration with S3 polymer solution\n",
    "hf_s3_poly_pyrid = root+'ES_PCL_PDO_270219/SEM/bead_free/compiled_data/radius/comb/'\n",
    "#Effect of Pyridine concentration with S4 polymer solution\n",
    "hf_s3_pyrid = root+'transport/SEM/compiled_data/hollow_fibre_s4_pyridine'\n",
    "\n",
    "#Flat sheet membrane micro-fibre analysis\n",
    "#Effect of Pyridine concentration with S4 polymer solution\n",
    "fs_s4_pyrid = root+'transport/SEM/compiled_data/flat_sheet_s4_pyridine/'\n",
    "#Effect of Flowrate with S4 polymer solution\n",
    "fs_s4_fr = root+'transport/SEM/compiled_data/flat_sheet_s4_flow_rate/'\n",
    "\n",
    "#test data location within each analysis set\n",
    "#test_dat_loc = analysis_set+'compiled_data/radius/test/'\n",
    "\n",
    "#Pierre biolig data\n",
    "bio_lig = root+'biolig/29_09_20/ImageJ/radius/'\n",
    "\n",
    "#List of all data sets to be analysed\n",
    "data_sets = [bio_lig]\n",
    "\n",
    "#sample key location\n",
    "sample_key_name = 'sample_key_17032020'\n",
    "sample_key = '/Users/ristomartin/Dropbox/UniStuff/DPhil/Experimental/sample_keys/'+sample_key_name+'.xlsx'\n",
    "\n",
    "#Import sample key\n",
    "sample_key = pd.read_excel(sample_key)\n",
    "\n",
    "#Set location of processed data\n",
    "processed = location+'/processed/'\n",
    "checkdir(processed) #check that processed data directory exists\n",
    "#location of processed histogram data\n",
    "hist_dists = processed+'hist_dist/'\n",
    "checkdir(hist_dists) #check that histogram data directory exists\n",
    "#location of processed recreated raw distribtions\n",
    "raw_dists = processed+'raw_dist/'\n",
    "checkdir(raw_dists) #check that processed recreated raw distribtion data directory exists\n",
    "#location of microfibre distribution figures\n",
    "#figure_loc = processed+'/'+matching+'/'+not_matching\n",
    "#checkdir(figure_loc) #check that figure directory exists\n",
    "#location of summary data\n",
    "#summary_loc = processed+'/summary_loc/'\n",
    "#checkdir(summary_loc) #check that figure directory exists\n",
    "\n",
    "#set variables to be considered to separate out distributions\n",
    "variables = ['mass_pcl','flow_rate'] #['solution_name','pyridine_conc','flow_rate','rotation_speed','poly_wall?']\n",
    "#create a dictionary of variable titles for each of the variables considered\n",
    "variable_label = {'solution_name':'Polymer Solution','pyridine_conc':'Pyridine Concentration (PPM)','wire_speed':'Wire Speed (mm/s)',\n",
    "                  'rotation_speed':'Rotation Speed (degrees/s)','flow_rate':'Flow Rate (ml/hr)','poly_wall?':'Polystyrene wall used',\n",
    "                 'mass_pcl':'PCL g/ml','flat_or_fibre':'flat_or_fibre'}  \n",
    "\n",
    "#Accepted R^2 value\n",
    "accept_r2 = 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## generation of raw data sets to be further processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initially specify all data sets of interest in data_sets list\n",
    "for dat_loc in data_sets:\n",
    "    #opening each file within each of the data sets to then be processed\n",
    "    for filename in os.listdir(dat_loc):\n",
    "        #only open the file if it end with the specified parameters as specified in file name\n",
    "        if filename.endswith(\".csv\"):\n",
    "            #print(filename)\n",
    "            #reads the specified directory and opens it as a dataframe\n",
    "            df = pd.read_csv(os.path.join(dat_loc,filename))\n",
    "\n",
    "            #Trimming df of rows of 0 frequency\n",
    "            df = df[df.Sum_of_Frequencies != 0]\n",
    "            #take all asociated diameters into a list\n",
    "            diameters = df['Radius']*2\n",
    "\n",
    "            #df_d finds the column headings by querying column by column in raw df to see if it ends with csv, as these are the files corresponding to each image\n",
    "            df_d = [col for col in df if col.endswith('csv')]\n",
    "            #copy all frequency columns across\n",
    "            df = df[df_d]\n",
    "            #calculate the normal mean and maintain series as pandas dataframe\n",
    "            df = pd.DataFrame((df.mean(axis = 1)/(df.mean(axis = 1).sum()))*100)\n",
    "\n",
    "            #give name to column with normal frequencies\n",
    "            df.columns = ['normal_frequency']\n",
    "            #add in column of diameters associated with each normal frequency \n",
    "            df['diameter'] = diameters\n",
    "            #print(df)\n",
    "\n",
    "            #add metadata to reconstrcuted data frame, only add to first line to minimise size of data frame\n",
    "            #for each of the variables considered\n",
    "            for variable in variables:\n",
    "                #add in the corresponding variables to the first line of the dataframe\n",
    "                df.loc[1, variable] = sample_key.loc[sample_key['filenamer'] == filename, variable].iloc[0]\n",
    "            #re-sort the dataframe according to it index\n",
    "            df = df.sort_index(axis=0)\n",
    "            #print(df)\n",
    "\n",
    "            #save each dataframe as a csv file so may be recalled later\n",
    "            df.to_csv(hist_dists+str(filename.strip('.csv'))+'.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Separation of distributions by variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initially create a dictionary of all files which have the same parameters used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'0.14,1.0': [424, 423], '0.1,1.0': [417, 418], '0.12,1.0': [421], '0.08,1.0': [425]}\n"
     ]
    }
   ],
   "source": [
    "para_comb = SimilarSetCollation(hist_dists,variables,sample_key,'filenamer','.csv')\n",
    "print(para_comb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combination of distributions with same parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create list of files in target directory\n",
    "target_files = [x for x in os.listdir(hist_dists) if x.endswith('csv')]\n",
    "#print(target_files)\n",
    "\n",
    "#Initially itterating through each of the different filename associated with each of the parameter combination identifiers\n",
    "for identifier in para_comb:\n",
    "    #create common dataframe for all data with common parameters\n",
    "    para_df = pd.DataFrame()\n",
    "    #initilise counter to keep track of how many files are being considered for each parameter set\n",
    "    file_count = 0\n",
    "    #create common DataFrame to pool all data of the samples with the same controlled variables along with summary values\n",
    "    complied_df = pd.DataFrame()\n",
    "    #create DataFrame to store all unique IDs\n",
    "    uni_ids = pd.DataFrame()\n",
    "    #cycle through each of the unique samples \n",
    "    for unique_id in para_comb[identifier]:\n",
    "        #add unique IDs to uni_ids DataFrame\n",
    "        uni_ids.loc[file_count,'uni_ids'] = unique_id\n",
    "        #retrieve file name associated with unique_id\n",
    "        filename = sample_key.loc[sample_key['unique_id'] == unique_id, 'filenamer'].iloc[0]\n",
    "        #remove any file extension from name\n",
    "        filename = filename.split('.')[0]\n",
    "        #print(filename)\n",
    "        #Check if filename is there or if need to add file extension or number to end\n",
    "        filenames = [s for s in target_files if filename in s]\n",
    "        #check any files found\n",
    "        if len(filenames) > 0:\n",
    "            #go through each of filenames in the directory associated with each of the identifiers with the same controlled variables\n",
    "            for name in filenames:\n",
    "                #print(name)\n",
    "                #open the dataframe associated with file name\n",
    "                df = pd.read_csv(hist_dists+name, index_col = 0)\n",
    "                #having opened file want to recreate original distribution\n",
    "                df = pd.DataFrame(sum([[row['diameter']] * int(round(row['normal_frequency'])) for index, row in df.iterrows()], []))\n",
    "                #append into common file to be collated\n",
    "                complied_df = complied_df.append(df)\n",
    "                #advance file counter\n",
    "                file_count = file_count + 1\n",
    "    \n",
    "    #print(complied_df.head())\n",
    "    \n",
    "    #reset the index to prevent shape and identifier missmatch problems later on\n",
    "    complied_df = complied_df.reset_index(drop=True)\n",
    "    #rename column by parameter identifier\n",
    "    complied_df = complied_df.rename(columns={0:'data'})\n",
    "    \n",
    "    #savefile out\n",
    "    complied_df.to_csv(raw_dists+str(unique_id)+'.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating directory of distributions which only vary by one variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def VariableSep2(data_set_dir,variables,sample_key):\n",
    "    #Import libraries\n",
    "    import os\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import itertools\n",
    "\n",
    "    #initially create two dictionaries the first of all the variables associated with their values paired with the unique ID\n",
    "    #Second Dictionary will contain each of the parameters and each of the variables values associated with it\n",
    "    #initilise the dictionary of all unique_ids and their associated parameter values\n",
    "    id_dic = {}\n",
    "    #initialise dictionary of every parameter value\n",
    "    parma_vals = {}\n",
    "    for filename in os.listdir(data_set_dir):\n",
    "        #initially open file to extract parameters\n",
    "        #only open the file if it end with the specified parameters as specified in file name\n",
    "        if filename.endswith(\".csv\"):\n",
    "            #extract uniquie Id from file name\n",
    "            uni_id = int(filename.split('.')[0])\n",
    "            # Create list of all variable parameter values\n",
    "            var_vals = []\n",
    "            for v in range(len(variables)):\n",
    "                #look up variable parameter value\n",
    "                val = variables[v]+'-'+str(sample_key.loc[sample_key['unique_id'] == uni_id, variables[v]].iloc[0])\n",
    "                #Append value variable value for each parameter considered to var_vals list of all variable parameter value\n",
    "                var_vals.append(val)\n",
    "                #add parameter and associated common identifier if does not already exist\n",
    "                if variables[v] not in parma_vals:\n",
    "                    parma_vals.setdefault(variables[v],[]).append(val)\n",
    "                #if parameter identifier already exisits, check if unique_id already in list and add if missing\n",
    "                elif val not in parma_vals[variables[v]]:\n",
    "                    parma_vals.setdefault(variables[v],[]).append(val)\n",
    "                #if already parameter key already exists and unique id in list then pass\n",
    "                else:\n",
    "                    pass\n",
    "            #Append key with associated values to id_dic with each unique id and their associated parameter values\n",
    "            id_dic.setdefault(uni_id,var_vals)\n",
    "    #print(id_dic)\n",
    "    #print(parma_vals)\n",
    "\n",
    "\n",
    "    #As only want to find combinations which match all but one variable, want to find combinations of n(variables) - 1\n",
    "    #As have to order matching variable combination create list of original order and the new ordering\n",
    "    var_com_order = {}\n",
    "    #To make matching combinations need to make dictionary with one less variable than total variables\n",
    "    #Create common dictionary for all n(variables - 1)\n",
    "    all_match_para_comb = []\n",
    "    #Initially want to cycle through all variables considered and cycle through list removing a different one each time\n",
    "    for i in range(len(variables)):\n",
    "        var_com = variables.copy()\n",
    "        var_com.pop(i)\n",
    "        #having produced a subset list of variables want to make a subset dictonary of all combinations\n",
    "        sub_id_dic = { your_key: parma_vals[your_key] for your_key in var_com }\n",
    "        #print(var_com)\n",
    "        #having produced sub dictionary of n(variables) - 1 now sort and make all combinations of them\n",
    "        allparamset = sorted(sub_id_dic)\n",
    "        #print(allparamset)\n",
    "        #add entry to dictionary of new order and old order to matching variable dictionary\n",
    "        #var_com_order.append((allparamset,var_com))\n",
    "        var_com_order.setdefault(str(allparamset),var_com)\n",
    "\n",
    "        combinations = list(set(list(itertools.product(*(sub_id_dic[paramset] for paramset in allparamset)))))\n",
    "        #print(combinations)\n",
    "        #having produced a dictonary of combinations of each of the variables want to merge into central dictionary to prevent repeats\n",
    "        all_match_para_comb.extend(combinations)\n",
    "    #Remove all duplicated combinations with set() and then convert to list be itterated through\n",
    "    all_match_para_comb = list(set(all_match_para_comb))\n",
    "\n",
    "    #print(all_match_para_comb)\n",
    "\n",
    "#######\n",
    "\n",
    "    #having produced a list of all combinations, all_match_para_comb, of n(variables) - 1 variables need to associate file names with each of these\n",
    "    #create a dictionary of matched combinations and their associated files\n",
    "    match_comb_dic = {}\n",
    "    #Having previously created dictionary of file names and associated parameters, cycle through each dictionary entry and check against matching params\n",
    "    for file in id_dic:\n",
    "        file_params = id_dic[file]\n",
    "        #print(type(id_dic[file]))\n",
    "        #print(id_dic[file])\n",
    "\n",
    "        #having converted filename into list now cycle through each of combinations of matching parameter values to check if applicable\n",
    "        for matching_params in all_match_para_comb:\n",
    "            #initially convert string into list to be compared\n",
    "            matching_params = list(matching_params)\n",
    "            #print(matching_params)\n",
    "            #print(type(matching_params))\n",
    "            #check if all elements in matching_params is in file name\n",
    "            check = all(item in file_params for item in matching_params)\n",
    "            #print(check)\n",
    "            if check is True:\n",
    "                #if combination of variables all found within file name then add to dictionary\n",
    "                #must first convert matching_params to string to be used as key\n",
    "                matching_params = str(matching_params)\n",
    "                #then can append file name to list associated with match_param combination\n",
    "                match_comb_dic.setdefault(matching_params,[]).append(file)\n",
    "                #print('match')\n",
    "            else:\n",
    "                pass\n",
    "    #print(match_comb_dic)\n",
    "\n",
    "\n",
    "    #create dictionary of sorted parameters\n",
    "    matched_para_comb = {}\n",
    "    #having produced a dictonary of all files associated with their matching parameters now need to spearate out by non-matching variable within each dictionary\n",
    "    for matching_params in match_comb_dic:\n",
    "      #print(matching_params)\n",
    "        #split matching_params into comma separated list, while look like is already it is actually just one stright (ffs)\n",
    "        matching_params_name = str(matching_params).replace(\"'\", '').replace(\" \", '').replace(\"[\", '').replace(\"]\", '').split(',')\n",
    "        #print(matching_params_name)\n",
    "        #having split into string of variable associated with parameter value now want to split into list\n",
    "        #create list of split variables names and variable values\n",
    "        split_params_name = []\n",
    "        #itterating through each variable and parameter value pair\n",
    "        for var in matching_params_name:\n",
    "            #split and extend to list of pairings\n",
    "            split_params_name.extend(var.split('-'))\n",
    "        #print(split_params_name)\n",
    "        #Recreate matching parameter name from split\n",
    "        matching = str(split_params_name).strip('[]').replace(\"'\",'')\n",
    "        #print(split_params_name)\n",
    "        #initially itterate through all filenames associated with each matching set of parameters\n",
    "        for filename in match_comb_dic[matching_params]:\n",
    "            #for each key want to retrieve corresponding parameter values from id_dic\n",
    "            param_set = id_dic[filename]\n",
    "            #for each filename parameter set want to remove all elements in they key from the file name to leave behind only the not matching variable\n",
    "            not_matching = str([i for i in param_set if i not in matching_params]).strip('[]').strip(\"'\")\n",
    "            #print(not_matching)\n",
    "            #Separate variable name and parameter value from one another\n",
    "            not_matching = not_matching.split('-')\n",
    "            #print(not_matching)\n",
    "            #set not matching to refer to variable only\n",
    "            not_matching = not_matching[0]\n",
    "            #print(matching)\n",
    "            #print(not_matching)\n",
    "            #append if filename is not present in list\n",
    "            if matching not in matched_para_comb.keys():\n",
    "                matched_para_comb[matching] = {not_matching:[filename]}\n",
    "                #print(str(matched_para_comb[matching][not_matching]))\n",
    "                #print('not there')\n",
    "            else:\n",
    "                #append if filename is not present in list\n",
    "                if filename not in matched_para_comb[matching][not_matching]:\n",
    "                    matched_para_comb[matching][not_matching].append(filename)\n",
    "                    #print(str(matched_para_comb[matching][not_matching]))\n",
    "                    #print('in ya go')\n",
    "                else: pass\n",
    "    #print(matched_para_comb)\n",
    "\n",
    "    #want to remove redundant dictionary keys containing only a single file or less\n",
    "    #because dictionary is likely to change length\n",
    "    #initially make list of first level keys to obtain key via index\n",
    "    matching_dict_keys = list(matched_para_comb.keys())\n",
    "    #print(type(matching_dict_keys))\n",
    "    #evaluate length of dictionaries\n",
    "    matching_len = len(matching_dict_keys)\n",
    "    #for each of the keys in the matching dictionary\n",
    "    for m in range(matching_len):\n",
    "        #print(matching_dict_keys[m])\n",
    "        #make list of second level keys to obtain key via index\n",
    "        not_matching_dict_keys = list(matched_para_comb[matching_dict_keys[m]].keys())\n",
    "        #print(type(not_matching_dict_keys)\n",
    "        #evaluate length of not matching keys in second level dictionary\n",
    "        not_matching_len = len(not_matching_dict_keys)\n",
    "        #for each of the keys in the second level dictoionary\n",
    "        for n in range(not_matching_len):\n",
    "            #print(not_matching_dict_keys[n])\n",
    "            #evaluate if there are less than two files in the second level dictionary\n",
    "            if len(matched_para_comb[matching_dict_keys[m]][not_matching_dict_keys[n]]) < 2:\n",
    "                #print('pop')\n",
    "                #if there is less than files in the second level dictionary delete the second level dictionary\n",
    "                del(matched_para_comb[matching_dict_keys[m]][not_matching_dict_keys[n]])\n",
    "\n",
    "                #matched_para_comb[matching_dict_keys[m]].pop(not_matching_dict_keys[n], None)\n",
    "            else:\n",
    "                #if there are more than two files in second level dictionary then leave second level dictionary\n",
    "                pass\n",
    "        #Re-evaluate length of first level dictionary\n",
    "        not_matching_len = len(list(matched_para_comb[matching_dict_keys[m]].keys()))\n",
    "        if not_matching_len < 1:\n",
    "            #print('POP')\n",
    "            #if there is less than one key in the first level dictionary then remove the first level dictionary\n",
    "            del(matched_para_comb[matching_dict_keys[m]])\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    #Check if only one variable used to separate\n",
    "    \n",
    "    \n",
    "    return(matched_para_comb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'flow_rate, 1.0': {'mass_pcl': [425, 418, 423, 421]}}\n"
     ]
    }
   ],
   "source": [
    "matched_para_comb = VariableSep2(raw_dists,variables,sample_key)\n",
    "print(matched_para_comb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating and deleting outputfiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "creation_and_destruction(processed,matched_para_comb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting of Data and summary data output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting characteristic y axis labels\n",
    "charactieristic_label = {'porosity':'Porosity (%)', 'max_wall_thickness':'Maximum membrane wall thickness ($\\mu$m)',\n",
    "                         'min_wall_thickness':'Minimum membrane wall thickness ($\\mu$m)'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combination of data sets based on matching parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Routing through each level of the dictionary initally parasing each of matching parameter sets\n",
    "for matching in matched_para_comb:\n",
    "    #strip out whitespace from matching to find directories\n",
    "    matchings = matching.replace(' ','') \n",
    "    #for each of the matching parameter sets route to the corresponding not matching parameters  \n",
    "    for not_matching in matched_para_comb[matching]:\n",
    "        #strip out whitespace from not_matching to find directories\n",
    "        not_matchings = not_matching.replace(' ','')\n",
    "        #print(not_matching)\n",
    "        #Make dataframe as holder for concatinated recreated data sets\n",
    "        con_df = pd.DataFrame()\n",
    "        #make dataframe for summary data\n",
    "        sum_df = pd.DataFrame()\n",
    "        #make list of not_matching variables for ordering\n",
    "        nm_order = []\n",
    "        #create file counter\n",
    "        file_count = 0\n",
    "        #in each of these not matching parameters parase all of the listed files\n",
    "        for filename in matched_para_comb[matching][not_matching]:\n",
    "            #print(matching+not_matching)\n",
    "            #maintian unique id associated with each data set\n",
    "            uni_id = filename\n",
    "            #convert uni_id to filename to allow opening of data set\n",
    "            filename = str(int(filename))+'.csv'\n",
    "            #open distribution data as dataframe\n",
    "            df = pd.read_csv(raw_dists+filename, index_col = 0)\n",
    "            #print(df.head())\n",
    "            \n",
    "            #extract not_matching parameter value from sample key\n",
    "            nm = sample_key.loc[sample_key['unique_id'] == uni_id, not_matching].iloc[0]\n",
    "            #append nm_order with not_matching parameter value\n",
    "            nm_order.append(nm)            \n",
    "            \n",
    "            #Calculate of the summary statistics\n",
    "            sum_df.loc[file_count,'median'] = np.percentile(df['data'].dropna(), 50)\n",
    "            sum_df.loc[file_count,'IQR'] = iqr(df['data'].dropna())\n",
    "            sum_df.loc[file_count,'25_quartile'] = np.percentile(df['data'].dropna(), 25)\n",
    "            sum_df.loc[file_count,'75_quartile'] = np.percentile(df['data'].dropna(), 75)\n",
    "            sum_df.loc[file_count,'mean'] = df['data'].mean()\n",
    "            sum_df.loc[file_count,'SD'] = df['data'].std()\n",
    "            skew = df['data'].skew()\n",
    "            sum_df.loc[file_count,'skew'] = skew\n",
    "            #from calculation of skew determine whether to use median or mean\n",
    "            if abs(skew) > 0.5:\n",
    "                sum_df.loc[file_count,'stat'] = 'median'\n",
    "            else:\n",
    "                sum_df.loc[file_count,'stat'] = 'mean'\n",
    "            sum_df.loc[file_count,'unique_id'] = uni_id\n",
    "            sum_df.loc[file_count,not_matching] = nm \n",
    "\n",
    "            #print(characteristics[c])\n",
    "            #Simultaniously want to extract raw characteristic data\n",
    "            #make dataframe of just extracted characteristic data\n",
    "            char_dat = df.filter(['data'], axis=1)\n",
    "            #rename the datacolumn to that of the not matching variable\n",
    "            char_dat = char_dat.rename(columns={'data':nm})\n",
    "            #print(char_dat.head())\n",
    "\n",
    "            #Concatinate each recreated data set to eachother           \n",
    "            con_df = pd.concat([con_df, char_dat], axis=1, sort=False)\n",
    "            #progress file count\n",
    "            file_count = file_count + 1\n",
    "        \n",
    "        #save out summary data\n",
    "        #print(sum_df.head())\n",
    "        sum_df.to_csv(processed+'/'+matchings+'/'+not_matchings+'/'+matchings+'_'+not_matchings+'_summary.csv')\n",
    "        #save out concatinated data\n",
    "        con_df.to_csv(processed+'/'+matchings+'/'+not_matchings+'/'+matchings+'_'+not_matchings+'_concatinated.csv')\n",
    "        \n",
    "        #convert multiple columns of data into single column with header as variable\n",
    "        #data = pd.melt(con_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Violin Plots of microfibre diameter distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flow_rate,1.0mass_pcl\n",
      "     0.08    0.1   0.14   0.12\n",
      "0  0.2410  0.625  0.250  0.625\n",
      "1  0.4820  1.250  0.250  1.250\n",
      "2  0.4820  1.250  0.375  1.250\n",
      "3  0.4820  1.250  0.375  1.250\n",
      "4  0.7228  1.250  0.375  1.250\n",
      "    variable    value\n",
      "0       0.08   0.2410\n",
      "1       0.08   0.4820\n",
      "2       0.08   0.4820\n",
      "3       0.08   0.4820\n",
      "4       0.08   0.7228\n",
      "..       ...      ...\n",
      "697     0.12   8.1250\n",
      "698     0.12   8.7500\n",
      "699     0.12   9.3750\n",
      "700     0.12  10.0000\n",
      "701     0.12  10.6250\n",
      "\n",
      "[692 rows x 2 columns]\n",
      "['0.08', '0.1', '0.12', '0.14']\n"
     ]
    }
   ],
   "source": [
    "#Routing through each level of the dictionary initally parasing each of matching parameter sets\n",
    "for matching in matched_para_comb:\n",
    "    #strip out whitespace from matching to find directories\n",
    "    matchings = matching.replace(' ','') \n",
    "    #for each of the matching parameter sets route to the corresponding not matching parameters  \n",
    "    for not_matching in matched_para_comb[matching]:\n",
    "        #strip out whitespace from not_matching to find directories\n",
    "        not_matchings = not_matching.replace(' ','') \n",
    "        #Make dataframe as holder for concatinated recreated data sets\n",
    "        con_df = pd.DataFrame()\n",
    "        #make dataframe for summary data\n",
    "        sum_df = pd.DataFrame()\n",
    "        #make list of not_matching variables for ordering\n",
    "        nm_order = []\n",
    "        \n",
    "        #open distribution data as dataframe\n",
    "        print(matchings+not_matchings)\n",
    "        df = pd.read_csv(processed+'/'+matchings+'/'+not_matchings+'/'+matchings+'_'+not_matchings+'_concatinated.csv', index_col = 0)\n",
    "        print(df.head())\n",
    "        \n",
    "        #convert multiple columns of data into single column with header as variable\n",
    "        data = pd.melt(df).dropna()\n",
    "        print(data)\n",
    "\n",
    "        #Find order of plots according to max value\n",
    "        #ordered = data.groupby(['variable'])['value'].aggregate(np.median).reset_index().sort_values('value')\n",
    "        #ordered_var = ordered['variable'].tolist()\n",
    "\n",
    "        #Find order of plots from magnitude of nm variable, determined by ordering nm_order\n",
    "        #first find all unique values in variable\n",
    "        nm_order = np.unique(data['variable'].values).tolist()\n",
    "        print(nm_order)\n",
    "        nm_order.sort(key = float, reverse=True)\n",
    "        #print(nm_order)\n",
    "\n",
    "        #create new template for figure\n",
    "        fig, ax = plt.subplots()\n",
    "        #plot violin plot into figure\n",
    "        v_plt = sns.violinplot( x='variable', y= 'value', cut=0, data = data, order = nm_order, ax = ax) #order = ordered_var,\n",
    "        \n",
    "        #insert statistical annotations\n",
    "        #before can add statistical annotation must create boxPairList from previous statistical comparison table\n",
    "        #set which variable list controls order\n",
    "        var_order = nm_order.copy()\n",
    "        #create list for boxpairlist\n",
    "        pre_boxPairList = []\n",
    "        #for count of number of o values\n",
    "        for index in range(len(var_order)):\n",
    "            #to ensure that all combinations are considered again copy the uniquevalues\n",
    "            avalues = var_order.copy() #colour hue\n",
    "            #removing fixed variable so only consider changing variables\n",
    "            avalues.remove(var_order[index])\n",
    "            #considereing the appending value\n",
    "            for index in range(len(avalues)):\n",
    "                #let a = the file name and the ovalue which corresponds to the number within the list and pair them\n",
    "                a = (avalues[index],var_order[index])\n",
    "                #add the pair to the list of boxed pairs\n",
    "                if avalues[index] != var_order[index]:\n",
    "                    if a not in pre_boxPairList:\n",
    "                        pre_boxPairList.append(a)\n",
    "                else: pass\n",
    "        #adding statistical annotation\n",
    "        if len(pre_boxPairList) > 1:\n",
    "            add_stat_annotation(x='variable', y= 'value', data = data, boxPairList=pre_boxPairList, \n",
    "                                test='Mann-Whitney', textFormat='star', loc='inside', verbose = 0,order = nm_order, ax = ax) #order = ordered_var,\n",
    "\n",
    "        #Set plot labels\n",
    "        #First add label to the x-axis to describe the variable considered\n",
    "        #Retreve xlabel axis associated with variable parameter\n",
    "        xlabel = variable_label[not_matching]\n",
    "\n",
    "        #Add correct x tick labels\n",
    "        #initially retreve the existing key labels\n",
    "        labels = [t.get_text()  for t in ax.get_xticklabels()]\n",
    "\n",
    "        xlabel_list = []\n",
    "        for label in labels:\n",
    "            #print(label)\n",
    "            #For polymer solution crossreference variable with name\n",
    "            if not_matching == 'solution_name':\n",
    "                #initilise dictionary of polymer solution names\n",
    "                polysolkey = {'0.0':'Trial','1.0':'Initial' ,'2.0':'S1','3.0':'S2','4.0':'S3','5.0':'S4','6.0':'S5'}\n",
    "                #change label to update polymer solution name\n",
    "                label = polysolkey[label]\n",
    "            elif not_matching == 'poly_wall?':\n",
    "                #initilise dictionary of polymer solution names\n",
    "                polysolkey = {'0.0':'No','1.0':'Yes'}\n",
    "                #change label to update polymer solution name\n",
    "                label = polysolkey[label]\n",
    "\n",
    "            #append list to list of variable labels\n",
    "            xlabel_list.append(label)\n",
    "        #using list of variable labels update x-axis tick labels\n",
    "        ax.set_xticklabels(xlabel_list)\n",
    "\n",
    "        #retrieving corrected y-axis label\n",
    "        ylabel = 'Micro-fibre diameter ($\\mu$m)'\n",
    "\n",
    "        #set the x and y axis labels\n",
    "        v_plt.set(xlabel=xlabel, ylabel= ylabel) #\n",
    "        #set the xlabels rotation\n",
    "        v_plt.set_xticklabels(v_plt.get_xticklabels(),rotation = -0)\n",
    "        #plt.show()\n",
    "\n",
    "        #save plot out\n",
    "        #save into directory for specific characteristic\n",
    "        fig_dir =processed+'/'+matchings+'/'+not_matchings+'/'\n",
    "        checkdir(fig_dir)\n",
    "\n",
    "        fig.savefig(fig_dir+matchings+'_'+not_matchings+'.png',bbox_inches='tight', dpi=300)\n",
    "\n",
    "        #Close figure to hide previews\n",
    "        plt.close(fig)\n",
    "        #print('done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fibre diameter as function of pyridine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Routing through each level of the dictionary initally parasing each of matching parameter sets\n",
    "for matching in matched_para_comb:\n",
    "    #strip out whitespace from matching to find directories\n",
    "    matchings = matching.replace(' ','') \n",
    "    #for each of the matching parameter sets route to the corresponding not matching parameters  \n",
    "    for not_matching in matched_para_comb[matching]:\n",
    "        #strip out whitespace from not_matching to find directories\n",
    "        not_matchings = not_matching.replace(' ','') \n",
    "        #Make dataframe as holder for concatinated recreated data sets\n",
    "        con_df = pd.DataFrame()\n",
    "        #make dataframe for summary data\n",
    "        sum_df = pd.DataFrame()\n",
    "        #make list of not_matching variables for ordering\n",
    "        nm_order = []\n",
    "        \n",
    "        #open distribution data as dataframe\n",
    "        df = pd.read_csv(processed+'/'+matchings+'/'+not_matchings+'/'+matchings+'_'+not_matchings+'_summary.csv', index_col = 0)\n",
    "        #print(df.head())\n",
    "   \n",
    "            \n",
    "            \n",
    "         #Using matpltlib to plot instead of seaborn to plot summary data\n",
    "        #print(not_matching)\n",
    "        if 'solution_name' in not_matching or 'flat_or_fibre' in not_matching :\n",
    "            pass\n",
    "        else:\n",
    "            ###Plotting contineous summary data and fit curve if there are more than 3 data points\n",
    "            if len(df[not_matching]) > 3:\n",
    "                #Initilise subplots \n",
    "                fig, ax = plt.subplots()\n",
    "                #print('pop')\n",
    "\n",
    "                #Using summary data table to extract data\n",
    "                x = df[not_matching].dropna()\n",
    "                xdata = np.asarray([1.0e-1 if x==0 else x for x in x])\n",
    "                ydata = np.asarray([1.0e-1 if x==0 else x for x in df['median'].dropna()])\n",
    "                yerr = ((df['median']-df['25_quartile']).dropna(),(df['75_quartile']-df['median']).dropna())\n",
    "\n",
    "                ax.errorbar(xdata, ydata, yerr=yerr, fmt='o')\n",
    "                ax.set(xlabel=xlabel, ylabel='Porosity (%)') #\n",
    "\n",
    "                #Set plot labels\n",
    "                #First add label to the x-axis to describe the variable considered\n",
    "                #Retreve xlabel axis associated with variable parameter\n",
    "                xlabel = variable_label[not_matching]\n",
    "\n",
    "                #retrieving corrected y-axis label\n",
    "                ylabel = 'Micro-fibre diameter ($\\mu$m)'\n",
    "\n",
    "                #set the x and y axis labels\n",
    "                ax.set(xlabel=xlabel, ylabel=ylabel) #\n",
    "                #set the xlabels rotation\n",
    "                #ax.set_xticklabels(ax.get_xticklabels(),rotation = -0)\n",
    "                #plt.show()\n",
    "\n",
    "                        ####Fit log curve with all scalars#####\n",
    "                yerr = np.asarray([1.0e-1 if x==0 else x for x in df['SD'].dropna()])\n",
    "                #using Scipy.optimise.curvefit then fit log function to the data to extract the curve parameters\n",
    "                #setting bounds to prevent -ve log values\n",
    "                bounds = (0,-np.inf, -np.inf), (np.inf, np.inf, np.inf)\n",
    "                #fit the curve\n",
    "                (m,b,c,r_squared,logcurvex,logcurvey) = logfit(xdata,ydata,yerr,bounds)\n",
    "                #plot\n",
    "                if r_squared > accept_r2:\n",
    "                    ax.plot(logcurvex,logcurvey,'r', linewidth=1)\n",
    "                else:\n",
    "                    pass\n",
    "\n",
    "                #saving lot fit data into summary table\n",
    "                #get first row index key \n",
    "                key = df.index[0]\n",
    "                #print(key)\n",
    "                df.loc[key,'m'] = m\n",
    "                df.loc[key,'b'] = b\n",
    "                df.loc[key,'c'] = c\n",
    "                df.loc[key,'log_r_squared'] = r_squared\n",
    "                #print(type(popt))\n",
    "\n",
    "                ##Fit linier function\n",
    "                #using Scipy.optimise.curvefit then fit lin function to the data to extract the curve parameters              \n",
    "                (m,c,r_squared,lincurvex,lincurvey) = linfit(xdata,ydata,yerr,bounds=None)\n",
    "\n",
    "                #plot if R^2 is significant\n",
    "                if r_squared > accept_r2:\n",
    "                    ax.plot(lincurvex,lincurvey,'b', linewidth=1)\n",
    "                else:\n",
    "                    pass\n",
    "\n",
    "                #saving lot fit data into summary table\n",
    "                #get first row index key \n",
    "                df.loc[key,'slope'] = m\n",
    "                df.loc[key,'intercept'] = c\n",
    "                df.loc[key,'lin_r_squared'] = r_squared\n",
    "\n",
    "                #save into directory for specific characteristic\n",
    "                fig_dir =processed+'/'+matchings+'/'+not_matchings+'/'\n",
    "                checkdir(fig_dir)\n",
    "\n",
    "                #save plot out\n",
    "                fig.savefig(fig_dir+matchings+'_'+not_matchings+'_microfibre_errorbarplt.png',bbox_inches='tight', dpi=300)\n",
    "                plt.close()\n",
    "\n",
    "                #saving updated summary dataframe out\n",
    "                #print(df)\n",
    "                df.to_csv(fig_dir+matchings+'_'+not_matchings+'_summary.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
